{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tajfsk_7JY3E"
   },
   "source": [
    "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). \n",
    "The total score must be re-scaled to 100 -- that should apply to all future assignments so that Canvas assigns the same weight on all assignments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPnHTf9MfT5X"
   },
   "outputs": [],
   "source": [
    "# Grader's area\n",
    "import numpy as np\n",
    "M = np.zeros([10,10])\n",
    "maxScore = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SArgW_Vq-uTh"
   },
   "source": [
    "# **Assignment 4**\n",
    "\n",
    "The goal of this assignment is to run some experiments with scikit-learn on a fairly sizeable and interesting image data set. This is the MNIST data set that consists of lots of images, each having 28x28 pixels. By today's standards, this may seem relatively tiny, but only a few years ago was quite challenging computationally, and it motivated the development of several ML algorithms and models that are now state-of-the-art  solutions for much bigger data sets. \n",
    "\n",
    "The assignment is experimental. We will try to whether a combination of PCA and kNN can yield any good results for the MNIST data set. Let's see if it can be made to work on this data set. \n",
    "\n",
    "Note: There are less difficult Python parts in this assignment. You can get things done by just repeating things from the class notebooks. But your participation and interaction via Canvas is always appreciated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTNaDFmoLus7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFM4hig-uTj"
   },
   "source": [
    "## Preparation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E3alYkjM-uTk"
   },
   "outputs": [],
   "source": [
    "# Import all necessary python packages\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import os\n",
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import ListedColormap\n",
    "#from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4JEBC9tZEZel"
   },
   "outputs": [],
   "source": [
    "# we load the data set directly from scikit learn \n",
    "# \n",
    "# note: this operation may take a few seconds. If for any reason it fails we \n",
    "# can revert back to loading from local storage. \n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "y = y.astype(int)\n",
    "X = ((X / 255.) - .5) * 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfrfDK0P-uT5"
   },
   "source": [
    "## <font color = 'blue'> Question 1. Inspecting the Dataset </font>\n",
    "\n",
    "**(i)** How many data points are in the training and test sets ? <br>\n",
    "**(ii)** How many attributes does the data set have ?\n",
    "\n",
    "Exlain how you found the answer to the first two questions. \n",
    "\n",
    "[**Hint**: Use the 'shape' method associated with numpy arrays. ]\n",
    "\n",
    "**(iii)** How many different labels does this data set have. Can you demonsrate how to read that number from the vector of labels *y_train*?  <br>\n",
    "**(iv)** How does the number of attributes relates to the size of the images? <br>\n",
    "**(v)** What is the role of line 12 in the above code? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*(Please insert cells below for your answers. Clearly id the part of the question you answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdlmoSSKL0HV"
   },
   "source": [
    "Q 1.1 Solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4po5m-tq-uT6",
    "outputId": "24305bfc-4560-476c-cf8a-ec048cbbe430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in training dataset =  60000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datapoints in training dataset = \", X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uBt0-MKYgAU",
    "outputId": "6297f2ed-f159-4bf6-b4ef-7c13d38c7878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in test dataset =  10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of datapoints in test dataset = \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EFY7YLCYlUI"
   },
   "source": [
    "Q 1.2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDugBGR3Ypno",
    "outputId": "ba8f5ce8-8f39-4c2d-8b57-0a0584392332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of attributes in X =  784\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of attributes in X = \", X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9dt8hkwYtrV"
   },
   "source": [
    "We observe that X_train has 60,000 rows and X_test has 10,000 rows. \n",
    "Each row represents 784 pixels in a 28x28 handwritten digit picture as attributes. \n",
    "Hence, we have 60,000 unique image samples in training and 10,000 unique image samples in test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBzrbkafYyEj"
   },
   "source": [
    "Q 1.3 Solution\n",
    "\n",
    "Let us check the unique number of elements in y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ok8ZjHqEYpkK",
    "outputId": "873c5cca-b681-4033-d8c8-1d28ca3c5edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique labels in y_train =  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique labels in y_train = \", y_train.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLtK_DEKYphj",
    "outputId": "0bd7fffe-8008-4e79-b51b-bbb76cbfd83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Those Unique labels are as follows:  [4 8 7 0 5 2 6 9 3 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nThose Unique labels are as follows: \", y_train.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE-I6ZoQZJ4P"
   },
   "source": [
    "Q 1.4 Solution\n",
    "\n",
    "We know that the size of one image sample in MNIST database is 28x28\n",
    "28x28 = 784\n",
    "The pixels in the image are reshaped into one long row (1x784) in our given dataset. \n",
    "X_train and X_test have 784 columns (attributes) which correspond to the 28x28 pixels in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6piApvhBZS4V"
   },
   "source": [
    "Q 1.5 Solution\n",
    "\n",
    "Initially in the fetched dataset from fetch_openml, they are grayscale images \n",
    "having pixel values ranging from 0-255. \n",
    "Where 0 generally is the darkest color gradient in the image and 255 is white color representation. \n",
    "Gradient based optimizations always work most stable when the data is scalled/ normalized. \n",
    "Hence, in order to scale the data between a close range [-1, 1] the below line is used.\n",
    "X = ((X / 255.) - .5) * 2\n",
    "For example if a pixel is 0,\n",
    "(0/255 - 0.5) * 2 = -1\n",
    "And if a pixel is 255,\n",
    "(255/255 - 0.5) * 2 = 1\n",
    "Hence all pixels in range [0, 255] will now be scaled between [-1, 1] which helps the modeling the data for better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnxFiBuKLxvq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IllLoXxGAIIo"
   },
   "outputs": [],
   "source": [
    "# For grader use only\n",
    "\n",
    "# in this case, make excetion and, assign 0-2 points for each subquestion\n",
    "\n",
    "# insert grade here  \n",
    "# G[1,1] = \n",
    "# G[1,2] =\n",
    "# G[1,3] = \n",
    "# G[1,4] = \n",
    "# G[1,5] =  \n",
    "\n",
    "\n",
    "maxScore = maxScore + 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMEcdAp3-uT-"
   },
   "source": [
    "##  <font color = 'blue'> Question 2. PCA on MNIST </font>\n",
    "\n",
    "Because the number of attributes of the MNIST data set may be too big to apply kNN on it (due to the 'curse of dimensionality'), we want to compress the images down to a smaller number of 'fake' attributes. \n",
    "\n",
    "Use scikit-learn to output a data set *X_train_transformed* and *X_test_transformed*, with $l$ attributes. Here a reasonable choice of $l$ is 10, equal to the number of labels. But you can try slightly smaller or bigger values as well. \n",
    "\n",
    "\n",
    "**Hint**: Take a look at [this notebook](https://colab.research.google.com/drive/1DG5PjWejo8F7AhozHxj8329SuMtXZ874?usp=drive_fs) we used in the lecture, and imitate what we did there. Be careful though, to use only the scikit-learn demonstration, not the exhaustive PCA steps we did before it.\n",
    "\n",
    "**Note**: This computation can take a while. If problems are encountered we can try the same experiment on a downsized data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0ek4Ry_-uT_"
   },
   "source": [
    " Q 2 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCfolqUfaUnK"
   },
   "source": [
    "Let us fit the pca with X_train, and use that to transform both x_train and x_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "174U17P4afVb"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10).fit(X_train)\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "X_test_transformed = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fATGH6onaiMc",
    "outputId": "9909f6d8-1b3d-4093-f134-6674fd2af868"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03437929,  5.93042898, -1.32557712, ...,  0.8019018 ,\n",
       "         0.79840204,  0.24801572],\n",
       "       [ 1.71107136,  0.61959259, -5.61616874, ..., -1.90999584,\n",
       "         4.2392874 ,  0.73459712],\n",
       "       [-3.7001538 ,  6.08597899, -2.9244742 , ...,  3.9178675 ,\n",
       "        -3.01684793,  1.97779327],\n",
       "       ...,\n",
       "       [-1.38016637, -7.54467952,  0.50226112, ...,  0.45941678,\n",
       "         0.99681634,  0.99236185],\n",
       "       [ 1.20194872,  7.38339737, -1.36960992, ..., -0.36764195,\n",
       "         4.36452479,  2.01714674],\n",
       "       [-1.08511158, -4.05124585, -6.87142438, ...,  5.19034749,\n",
       "        -1.13512276, -0.03783238]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0322FQFYalQ1",
    "outputId": "ba472162-c1b3-4842-8430-9a4540d69a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.03519080e+00, -1.98450372e+00,  1.13102652e+00, ...,\n",
       "        -8.75749497e-02,  3.00985524e+00, -1.38450237e+00],\n",
       "       [-2.40049602e+00,  6.22497703e+00,  3.36925073e-03, ...,\n",
       "        -9.45233276e-01, -1.76151956e+00,  1.41992121e+00],\n",
       "       [ 3.00133262e+00,  1.35338249e+00, -1.65937150e-01, ...,\n",
       "         2.03883549e+00, -2.90634752e-01, -7.43722886e-01],\n",
       "       ...,\n",
       "       [ 5.16858165e+00, -3.04103594e+00,  2.89070441e+00, ...,\n",
       "         5.93610708e+00, -4.42720363e+00, -1.31170901e+00],\n",
       "       [-5.94360770e+00, -1.19969025e+00, -1.18195932e-01, ...,\n",
       "        -2.89865177e-01, -2.00405449e+00, -2.30484225e+00],\n",
       "       [-4.71396056e+00, -8.16037076e-01, -1.46922238e+00, ...,\n",
       "        -9.14891156e-01, -3.15507579e+00,  3.32986193e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBjMZF1wGaUp"
   },
   "outputs": [],
   "source": [
    "# for grader use\n",
    "maxScore = maxScore +4 \n",
    "\n",
    "\n",
    "# insert grade here (out of 4)\n",
    "# G[2,1] =\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe9kKR3J-uUA"
   },
   "source": [
    "## <font color = 'blue'> Question 3. kNN on MNIST attributes from PCA </font>\n",
    "\n",
    "\n",
    "Having calculated the *transformed* MNIST data set we can now apply a kNN approach to the MNIST classification data set. Here are the sets:\n",
    "\n",
    "(i) Fit a $k$-NN classifier on the transformed data set. Here $k$ is a hyperparameter, and you can experiment with it. Be aware though, that larger $k$ can take more time to fit. \n",
    "\n",
    "(ii) Apply the classifier on the transformed test set. What is the classification accuracy? \n",
    "\n",
    "(iii) A theoretical question: if we skipped all the above steps and we just assigned a **random** label to each test point, what would the classification accuracy be on average?  Does your result (ii) beat the random expectation? \n",
    "\n",
    "(iv) Experiment with different settings of $k$, and other hyperparameters that are described in the scikit-learn manual of the kNN classifier. Report your findings in a separate cell. Also for **participation points**: report your best result on Canvas! \n",
    "\n",
    "[**Hint**: Imitate the steps from the classroom notebook]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7swRSuoNa95n"
   },
   "source": [
    "Q 3.1  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5v7Q2NKp-uUM"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EmviZuo2bLLa"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6WzU6g7bUse"
   },
   "source": [
    "Q 3.2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zOUAxzkybMl3"
   },
   "outputs": [],
   "source": [
    "y_pred_train = knn.predict(X_train_transformed)\n",
    "y_pred_test = knn.predict(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PsBhveRhbMiT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVOYSGdIbMfb",
    "outputId": "6a123992-cc45-4d4c-8665-e88454640cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9532166666666667\n",
      "\n",
      "Testing Accuracy =  0.9347\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd7fAgtEbe9U"
   },
   "source": [
    "Q 3.3 Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7x9RHJiDbmat"
   },
   "source": [
    "If we replaced every unique label with another random label consistently in both train and test datasets, \n",
    "For example, \n",
    "If all images of hand written digit 4 were to be replaced with a random label 91 (consistantly for all 4, replace 91).\n",
    "And similarly for all labels, \n",
    "then the accuracy would not change. Because KNN classifier segregates the data samples based on how similar they are,\n",
    "and checks if they have same label or not. As long as they are consistantly replaced with random variable the accuracy would not change. \n",
    "However, if every test sample is mapped with a completely random label. Then the classifier accuracy would drastically reduce.\n",
    "KNN classifer would still segregate similar datapoints into clusters, and when checked their labels would be very different, hence the accuracy \n",
    "would be very low (Mostly accuracy = 0, unless there are accidental true positives while randomly assigning the label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLHiT4mQbs2L"
   },
   "source": [
    "Q 3.4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKbfXx0NbMcf",
    "outputId": "c6bc0c2d-0507-4a68-fc94-bc84829952cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9532166666666667\n",
      "\n",
      "Testing Accuracy =  0.9347\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train_transformed)\n",
    "y_pred_test = knn.predict(X_test_transformed)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOAmyvR7dAAA",
    "outputId": "a3c004ff-8198-4c2f-c220-3634f2a1f06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9616333333333333\n",
      "\n",
      "Testing Accuracy =  0.9321\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='kd_tree', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train_transformed)\n",
    "y_pred_test = knn.predict(X_test_transformed)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3qVFHGrdCrM",
    "outputId": "63b84a02-537c-4143-c387-0b64dde779e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9445333333333333\n",
      "\n",
      "Testing Accuracy =  0.9329\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train_transformed)\n",
    "y_pred_test = knn.predict(X_test_transformed)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vfDZPgZ5c_0X",
    "outputId": "e71b3bdb-3dbe-4c32-ad78-2608781091aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9490833333333333\n",
      "\n",
      "Testing Accuracy =  0.9344\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train_transformed)\n",
    "y_pred_test = knn.predict(X_test_transformed)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYeqXEXtep7z",
    "outputId": "84f60d70-1805-4e92-ce99-5aa93d6f0e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9323833333333333\n",
      "\n",
      "Testing Accuracy =  0.9276\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=25, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train_transformed)\n",
    "y_pred_test = knn.predict(X_test_transformed)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "\n",
    "My best accuracy so far after optimizing the model is ~93.5% on the PCA transformed test set. \n",
    "\n",
    "Out of curiosity I tried training and using the non-transformed data. It took a longer period of time to train but was able to get around 97.48% accuracy on the test set after optimizing the model. \n",
    "\n",
    "I guess this makes sense, because we loose information during dimensionality reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy =  0.9809833333333333\n",
      "\n",
      "Testing Accuracy =  0.9748\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn = knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = knn.predict(X_train)\n",
    "y_pred_test = knn.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Accuracy = \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"\\nTesting Accuracy = \", accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuYcPkCaGe9p"
   },
   "outputs": [],
   "source": [
    "# for grader use\n",
    "maxScore = maxScore +12\n",
    "\n",
    "# insert grade here (each item out of 4)\n",
    "# G[3,1] =\n",
    "# G[3,2] = \n",
    "# G[3,3] =\n",
    "# G[3,4] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fydXo8GRGkbp"
   },
   "outputs": [],
   "source": [
    "# for grader use\n",
    "\n",
    "# Total Grade Calculation\n",
    "rawScore = np.sum(G)\n",
    "score = rawScore*100/maxScore"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
